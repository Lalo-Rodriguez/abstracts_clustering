{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b5663a-4739-4e84-a432-8eb0b6431c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import xmltodict\n",
    "import json\n",
    "from tqdm import tqdm  # Optional: for progress tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e78dc54-44d5-48cf-8e89-69829e01540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://www.nsf.gov/awardsearch/download?DownloadFileName=2020&All=true\"\n",
    "file_name = \"2020.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44636399-4a25-4d2d-b902-463ae34ea8c3",
   "metadata": {},
   "source": [
    "# Download the zip file, unzip, and create the Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728883dc-dcb3-4985-bcbe-bcc6e885c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download the file\n",
    "def download_file(url, download_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(download_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(f\"File downloaded to {download_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ecf2212-c0be-4cd1-9924-12ac6fab42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Move the file\n",
    "def move_file(src_path, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "    dest_path = os.path.join(dest_folder, os.path.basename(src_path))\n",
    "    shutil.move(src_path, dest_path)\n",
    "    print(f\"File moved to {dest_path}\")\n",
    "    return dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5274e548-928d-492d-bc80-dce069f47e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Unzip the file\n",
    "def unzip_and_delete_downloaded_file(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"File {zip_path} unzipped to {extract_to}\")\n",
    "    os.remove(zip_path)\n",
    "    print(f\"File {zip_path} deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7efb240e-11b6-4c86-b92e-4a22dc8bd4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Unify the data into a json file\n",
    "def unify_file(data_path, dest_folder):\n",
    "    if not os.path.exists(dest_folder):\n",
    "        os.makedirs(dest_folder)\n",
    "\n",
    "    # List to store JSON data from all XML files\n",
    "    all_data = []\n",
    "    \n",
    "    # Loop through each file in the folder\n",
    "    for xml_file_name in tqdm(os.listdir(data_path), desc=\"Processing XML files\"):\n",
    "        if xml_file_name.endswith('.xml'):\n",
    "            xml_file_path = os.path.join(data_path, xml_file_name)\n",
    "            \n",
    "            # Read and parse the XML file\n",
    "            with open(xml_file_path, 'r', encoding='utf-8') as xml_file:\n",
    "                xml_content = xml_file.read()\n",
    "                try:\n",
    "                    # Convert XML to a dictionary\n",
    "                    xml_dict = xmltodict.parse(xml_content)\n",
    "                    \n",
    "                    # Append the parsed XML data (as a dictionary) to the list\n",
    "                    all_data.append(xml_dict)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {xml_file_name}: {e}\")\n",
    "\n",
    "    # Write the accumulated data to a single JSON file\n",
    "    dest_path = os.path.join(dest_folder, 'data.json')\n",
    "    with open(dest_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"File {dest_path} created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527fa4c7-3e93-4756-96d3-5be157f612f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating the entire process\n",
    "def automate_download_and_unzip(url, file_name):\n",
    "    # Define paths\n",
    "    download_folder = os.path.expanduser(\"~/Downloads\")  # Adjust this if needed\n",
    "    download_file_name = \"nsf_download.zip\"\n",
    "    download_path = os.path.join(download_folder, file_name)\n",
    "    \n",
    "    data_folder = \"data\"  # Change this to your desired folder path\n",
    "    extract_to = os.path.join(data_folder, \"unzipped_files\")  # Destination folder for unzipped content\n",
    "    \n",
    "    # Step 1: Download the file\n",
    "    download_file(url, download_path)\n",
    "    time.sleep(10)  # Adjust time if needed\n",
    "       \n",
    "    # Step 2: Move the file\n",
    "    moved_path = move_file(download_path, data_folder)\n",
    "    \n",
    "    # Step 3: Unzip the file\n",
    "    unzip_and_delete_downloaded_file(moved_path, extract_to)\n",
    "\n",
    "    # Step 4: Unify the downloads into a Json file\n",
    "    unify_file(extract_to, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85a471-b540-4e78-9ea0-3fd10b5ff45c",
   "metadata": {},
   "source": [
    "# Read the json file and create the unique abstract list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6f13ba-225b-4660-a74e-1e4be58cf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field, AliasPath\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f1f84e8-1060-46d3-b2a0-6a52d9b99d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModel(BaseModel):\n",
    "    award_id: Optional[int] = Field(validation_alias=AliasPath('rootTag', 'Award', 'AwardID'))\n",
    "    award_title: str = Field(validation_alias=AliasPath('rootTag', 'Award', 'AwardTitle'))\n",
    "    abstract: Optional[str] = Field(validation_alias=AliasPath('rootTag', 'Award', 'AbstractNarration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab404d5c-8b28-4cb2-804d-30958524a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_abstract_list(json_file = 'data/data.json'):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    model = [DataModel(**item).dict(exclude_none=True) for item in data]\n",
    "    \n",
    "    abstracts_set = {entry.get('abstract') for entry in model if entry.get('abstract') and entry.get('abstract').strip()}\n",
    "    unique_abstract_list = list(abstracts_set)\n",
    "    return unique_abstract_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef013920-beab-4f33-8a2e-33225f108c85",
   "metadata": {},
   "source": [
    "# Create the stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ffa888b-94d8-49db-bc4d-3aa3c4bdc787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57b81284-6192-45b3-ac2a-717e105ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_stopwords = [stemmer.stem(w) for w in stopwords]\n",
    "stemmed_stopwords_set = {entry for entry in stemmed_stopwords}\n",
    "unique_stemmed_stopwords = list(stemmed_stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb745e3-aa30-4db1-b244-e2357c27abf2",
   "metadata": {},
   "source": [
    "# Preprocessing the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b8b810-d751-4ae6-a2d1-bf3e6251411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1bcc2b1-5e00-4ed9-a496-0117fa6f6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stop_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Use regex to remove specific substrings\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)  # Remove <br/> or <br>\n",
    "    text = re.sub(r'&lt;br/&gt;', ' ', text)  # Remove &lt;br/&gt;\n",
    "    text = re.sub(r\"This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria\", ' ', text)\n",
    "    \n",
    "    # Tokenize and lowercase\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter out stop words before lemmatization\n",
    "    filtered_before_lemmatization = [w for w in word_tokens if w.isalpha() and w not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in filtered_before_lemmatization]\n",
    "    \n",
    "    # Filter out stop words after lemmatization\n",
    "    filtered_after_lemmatization = [w for w in lemmatized_text if w not in stop_words]\n",
    "    \n",
    "    # Join processed words into final text\n",
    "    processed_text = ' '.join(filtered_after_lemmatization)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccafe9d-f417-44a0-b48c-a708c2261b2f",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e5bf54-3701-41e9-9221-236f8dca33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12cc8ef-91a5-4518-984b-fa9287e78286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_matrix(n_features, text):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    matrix  = vectorizer.fit_transform(preprocessed_abstracts)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec9fd1-2689-403a-a607-a07a6f985853",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231440ce-086a-4d59-9799-0c43321306bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bc84bf-d918-484b-9aa5-6f1cadec083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(tfidf_matrix, n_components, num_clusters):\n",
    "    svd = TruncatedSVD(n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    \n",
    "    X = lsa.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Using TSNE to reduce dimensionality to 2 for visualization\n",
    "    X = TSNE(n_components=2).fit_transform(X)\n",
    "\n",
    "    clusterer = KMeans(n_clusters=num_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    clustered_data = {}\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        if label not in clustered_data:\n",
    "            clustered_data[label] = []\n",
    "        clustered_data[label].append(preprocessed_abstracts[i])\n",
    "    \n",
    "    return clustered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d7b55-b463-4e32-996b-6555c250fb0c",
   "metadata": {},
   "source": [
    "# Get top words for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8370bafa-c10d-48cf-9087-2844908740f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3059ac91-09ae-4e5f-b222-9ad95a49196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(cluster_texts, n_top_words):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(cluster_texts)\n",
    "    word_counts = np.asarray(X.sum(axis=0)).flatten()  # Get word counts across all texts\n",
    "    vocab = vectorizer.get_feature_names_out()  # List of words (vocabulary)\n",
    "    \n",
    "    # Get top words\n",
    "    word_freq = [(word, word_counts[idx]) for idx, word in enumerate(vocab)]\n",
    "    top_words = sorted(word_freq, key=lambda x: x[1], reverse=True)[:n_top_words]\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050e25a-10aa-4608-89fe-2981c1ea0ed7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89411cda-126d-4fb9-b998-aaece50e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f167da-d9ca-465c-aed1-0f8d2f62dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'data/data.json'\n",
    "\n",
    "custom_stop_words = ['project', 'data', 'research', 'student', 'award', 'program', 'using', 'impact', 'new', 'support', 'nsf', 'foundation', 'study', 'science', 'use', 'develop', 'development']\n",
    "stop_words = set(stopwords.words('english')).union(custom_stop_words)\n",
    "\n",
    "n_features = 1000\n",
    "\n",
    "n_components = 100\n",
    "num_clusters = 5\n",
    "\n",
    "cluster_keywords = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e903648-3c58-454a-bb98-9f367f8e1257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to C:\\Users\\Eduardo Rodriguez/Downloads\\2020.zip\n",
      "File moved to data\\2020.zip\n",
      "File data\\2020.zip unzipped to data\\unzipped_files\n",
      "File data\\2020.zip deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing XML files: 100%|██████████| 13300/13300 [01:55<00:00, 115.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data\\data.json created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing the abstracts: 100%|██████████| 11507/11507 [00:41<00:00, 279.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 words in Cluster 3:\n",
      "model: 3303\n",
      "learning: 2949\n",
      "application: 2563\n",
      "problem: 2416\n",
      "method: 2145\n",
      "algorithm: 2030\n",
      "theory: 1941\n",
      "design: 1857\n",
      "technology: 1675\n",
      "technique: 1626\n",
      "tool: 1540\n",
      "analysis: 1507\n",
      "approach: 1437\n",
      "machine: 1408\n",
      "computing: 1396\n",
      "work: 1393\n",
      "network: 1385\n",
      "information: 1354\n",
      "goal: 1230\n",
      "provide: 1227\n",
      "\n",
      "Top 20 words in Cluster 1:\n",
      "material: 5967\n",
      "quantum: 3419\n",
      "energy: 2735\n",
      "property: 2403\n",
      "technology: 2241\n",
      "process: 2226\n",
      "model: 2169\n",
      "structure: 2010\n",
      "understanding: 1990\n",
      "application: 1897\n",
      "device: 1864\n",
      "high: 1857\n",
      "design: 1800\n",
      "field: 1775\n",
      "used: 1765\n",
      "chemical: 1679\n",
      "undergraduate: 1574\n",
      "method: 1552\n",
      "fundamental: 1532\n",
      "physic: 1519\n",
      "\n",
      "Top 20 words in Cluster 2:\n",
      "cell: 3215\n",
      "model: 2715\n",
      "water: 2586\n",
      "change: 2407\n",
      "understanding: 2223\n",
      "process: 2101\n",
      "plant: 2077\n",
      "specie: 1906\n",
      "protein: 1881\n",
      "community: 1877\n",
      "ocean: 1781\n",
      "provide: 1735\n",
      "climate: 1603\n",
      "used: 1515\n",
      "undergraduate: 1487\n",
      "result: 1403\n",
      "work: 1361\n",
      "method: 1267\n",
      "gene: 1243\n",
      "training: 1232\n",
      "\n",
      "Top 20 words in Cluster 0:\n",
      "stem: 5738\n",
      "learning: 3838\n",
      "education: 3319\n",
      "teacher: 2717\n",
      "engineering: 2596\n",
      "community: 2342\n",
      "school: 2062\n",
      "university: 2010\n",
      "experience: 1987\n",
      "undergraduate: 1984\n",
      "faculty: 1901\n",
      "career: 1783\n",
      "practice: 1701\n",
      "field: 1627\n",
      "professional: 1620\n",
      "college: 1568\n",
      "technology: 1504\n",
      "provide: 1467\n",
      "conference: 1410\n",
      "course: 1393\n",
      "\n",
      "Top 20 words in Cluster 4:\n",
      "social: 2592\n",
      "network: 2532\n",
      "model: 2520\n",
      "community: 2381\n",
      "health: 2059\n",
      "change: 1758\n",
      "public: 1627\n",
      "understanding: 1566\n",
      "disease: 1489\n",
      "provide: 1487\n",
      "information: 1471\n",
      "pandemic: 1415\n",
      "technology: 1373\n",
      "human: 1371\n",
      "work: 1315\n",
      "approach: 1297\n",
      "risk: 1293\n",
      "response: 1240\n",
      "analysis: 1226\n",
      "result: 1220\n"
     ]
    }
   ],
   "source": [
    "# Download the zip file, unzip, and create the Json file\n",
    "if not os.path.isfile(json_file):\n",
    "    automate_download_and_unzip(data_url, file_name)   \n",
    "\n",
    "# Read the json file and create the unique abstract list\n",
    "unique_abstracts = create_unique_abstract_list(json_file)\n",
    "preprocessed_abstracts = [preprocess_text(a, stop_words) for a in tqdm(unique_abstracts, desc=\"Preprocessing the abstracts\")]\n",
    "tfidf_matrix = get_tfidf_matrix(n_features, preprocessed_abstracts)\n",
    "clustered_data = get_clusters(tfidf_matrix, n_components, num_clusters)\n",
    "\n",
    "for cluster_id, texts in clustered_data.items():\n",
    "    print(f\"\\nTop {cluster_keywords} words in Cluster {cluster_id}:\")\n",
    "    top_words = get_top_words(texts, cluster_keywords)\n",
    "    for word, freq in top_words:\n",
    "        print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e6c40-398a-4ade-b1c2-6866a811e695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
